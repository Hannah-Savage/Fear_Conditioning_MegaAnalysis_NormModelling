{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3982a850-1c4b-400b-8828-02d5a699469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/preclineu/hansav/.conda/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pcntoolkit.dataio.fileio import load as ptkload\n",
    "from pcntoolkit.dataio.fileio import save as ptksave\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib as mpl\n",
    "from pathlib  import Path\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\", 9)\n",
    "\n",
    "# globals\n",
    "root_dir = '/project_cephfs/3022017.06/ENIGMA_ANX/'\n",
    "proc_dir = os.path.join(root_dir,'Z_stat/')\n",
    "data_dir = os.path.join(proc_dir,'data/')\n",
    "w_dir = os.path.join(proc_dir,'vox/')\n",
    "save_dir = os.path.join(w_dir,'Validation/')\n",
    "mask_nii = ('/opt/fmriprep/templateflow/tpl-MNI152NLin2009cAsym/tpl-MNI152NLin2009cAsym_res-02_desc-brain_mask.nii.gz')\n",
    "ex_nii = os.path.join(data_dir, 'ENIGMA_FC_tr_1.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93a703d-febf-47c3-b0f2-4699a3af3887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depression data available for: 152 people\n",
      "[3 2 10 2 0 8 1 11 2 1 5 3 7 11 9 9 12 9 2 11 21 0 19 31 45 20 24 31 41 13\n",
      " 17 24 24 35 32 46 10 14 25 43 24 18 27 37 23 14 23 11 20 7 9 5 1 3 0 6 1\n",
      " 5 0 0 1 7 0 2 1 1 3 16 0 0 0 0 5 0 1 0 0 1 0 2 5 0 5 0 1 0 1 4 7 0 2 0 0\n",
      " 7 36 20 20 13 44 13 6 39 9 16 20 8 34 17 32 23 34 23 0 7 29 21 22 17 20\n",
      " 21 7 3 3 19 16 26 6 11 9 21 31 9 1 26 41 16 28 24 18 23 37 14 27 19 18 0\n",
      " 4 6 20 32 13 12]\n"
     ]
    }
   ],
   "source": [
    "# Load in the Z_est files\n",
    "Z_est_control_test = ptkload(os.path.join(w_dir,'Z_estimate.pkl'), mask=mask_nii)\n",
    "Z_est_clinical = ptkload(os.path.join(w_dir,'Z_predcl.pkl'), mask=mask_nii)\n",
    "Full_sample_deviations = np.append(Z_est_control_test,Z_est_clinical, axis = 0)\n",
    "\n",
    "#Load in the contingency awareness data\n",
    "Measures = pd.read_csv('/project_cephfs/3022017.06/ENIGMA_ANX/Z_stat/data/all_test_validation.csv', usecols = [\"Group_Dataset\", \n",
    "                                                                                                                \"Anxiety_instrument\",\n",
    "                                                                                                                \"Anxiety_score\",\n",
    "                                                                                                                \"Depression_instrument\",\n",
    "                                                                                                                \"Depression_score\", \n",
    "                                                                                                                'Principal_diagnosis_current']) \n",
    "Measures.replace(to_replace='NA/does not apply', value='NA', regex=True, inplace=True)\n",
    "\n",
    "Measures['Anxiety_score'] = pd.to_numeric(Measures['Anxiety_score'], errors='coerce').astype('Int64')\n",
    "Measures['Depression_score'] = pd.to_numeric(Measures['Depression_score'], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "# FOR STAI- SPANISH VERSION: add 20 to make scale the same as others:\n",
    "spanish_STAI_sites = [\"Barcelona_Cardoner\", \"Barcelona_Soriano_dataset_1\", \"Barcelona_Soriano_dataset_2\"]\n",
    "# Check if instrument_value is 1 and the Group_Dataset is in the specified list\n",
    "for index, row in Measures.iterrows():\n",
    "    if row['Group_Dataset'] in spanish_STAI_sites and Measures.at[index, 'Anxiety_instrument'] == 'stai-trait':\n",
    "        # Modify Anxiety_score by adding 20\n",
    "        Measures.at[index, 'Anxiety_score'] += 20\n",
    "\n",
    "\n",
    "#Mask by participants for whom CA data is available\n",
    "#mask_Anx = Measures[\"Anxiety_score\"].notna() #remove NAs\n",
    "#mask_select_measure = Measures['Anxiety_instrument'].isin(['stai-trait']) #CHANGE THIS TO QUESTIONNIARE OF INTEREST\n",
    "#beck anxiety inventory\n",
    "#hamilton anxiety\n",
    "#other (scared total score)\n",
    "#other (staic)\n",
    "#stai-state\n",
    "#stai-trait\n",
    "\n",
    "mask_exclude_diagnosis = ~Measures['Principal_diagnosis_current'].isin(['others', 'schizophrenia']) #and remove others and schizophrenia\n",
    "combined_mask_Anx = mask_Anx & mask_exclude_diagnosis \n",
    "combined_mask_Anx = combined_mask_Anx & mask_select_measure\n",
    "\n",
    "#Anx_sample = Measures['Anxiety_score'][combined_mask_Anx].to_numpy()\n",
    "#Anx_sample_deviations = Full_sample_deviations[combined_mask_Anx]\n",
    "#print('Anxiety data available for: '+str(len(Anx_sample)) +' people')\n",
    "#print(Anx_sample)\n",
    "\n",
    "mask_Dep = Measures[\"Depression_score\"].notna() #remove NAs\n",
    "mask_select_measure = Measures['Depression_instrument'].isin(['beck depression inventory']) #CHANGE THIS TO QUESTIONNIARE OF INTEREST\n",
    "combined_mask_Dep = mask_Dep & mask_exclude_diagnosis\n",
    "combined_mask_Dep = combined_mask_Dep & mask_select_measure\n",
    "\n",
    "Dep_sample = Measures['Depression_score'][combined_mask_Dep].to_numpy()\n",
    "Dep_sample_deviations = Full_sample_deviations[combined_mask_Dep]\n",
    "print('Depression data available for: '+str(len(Dep_sample)) +' people')\n",
    "print(Dep_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e7f172-639c-4309-b711-4483376a9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "X1 = Dep_sample_deviations #Deviations\n",
    "y = Dep_sample.ravel()\n",
    "n_samples, n_features = X1.shape\n",
    "random_state = np.random.RandomState(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a6023f2-93cb-40eb-9917-9265688efa6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "1.0\n",
      "12.750799103385464\n",
      "R2:-0.210, MSE:203.38, RMSE:14.26\n",
      "Iteration 1\n",
      "1.0\n",
      "13.27001860342568\n",
      "R2:-0.118, MSE:103.51, RMSE:10.17\n",
      "Iteration 2\n",
      "1.0\n",
      "13.111495577955168\n",
      "R2:-0.266, MSE:191.50, RMSE:13.84\n",
      "Iteration 3\n",
      "1.0\n",
      "13.0836246378012\n",
      "R2:0.030, MSE:211.11, RMSE:14.53\n",
      "Iteration 4\n",
      "1.0\n",
      "13.773250833183967\n",
      "R2:-0.609, MSE:100.34, RMSE:10.02\n",
      "Iteration 5\n",
      "1.0\n",
      "12.842968952696364\n",
      "R2:-0.273, MSE:146.05, RMSE:12.09\n",
      "Iteration 6\n",
      "1.0\n",
      "13.36276132901337\n",
      "R2:-0.480, MSE:161.45, RMSE:12.71\n",
      "Iteration 7\n",
      "1.0\n",
      "13.51145291579118\n",
      "R2:0.131, MSE:105.11, RMSE:10.25\n",
      "Iteration 8\n",
      "1.0\n",
      "13.346168140157216\n",
      "R2:-0.934, MSE:150.08, RMSE:12.25\n",
      "Iteration 9\n",
      "1.0\n",
      "12.7491870408895\n",
      "R2:0.157, MSE:112.00, RMSE:10.58\n"
     ]
    }
   ],
   "source": [
    "#%%ELASTIC NET CV\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "model_alpha = []\n",
    "model_intercept = []\n",
    "\n",
    "ypred_cf = []\n",
    "score_cf = []\n",
    "ev_cf = []\n",
    "mse_cf = []\n",
    "coefs_cf = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    print('Iteration',i)\n",
    "\n",
    "    alphas = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X1, y, test_size=0.1)\n",
    "    \n",
    "    #ElasticNetCV is a cross-validation class that can search multiple alpha values \n",
    "    #and applies the best one. We'll define the model with alphas value and fit \n",
    "    #it with xtrain and ytrain data.\n",
    "    \n",
    "    elastic_cv=ElasticNetCV(alphas=alphas, cv=5)\n",
    "    model = elastic_cv.fit(xtrain, ytrain)\n",
    "    \n",
    "    print(model.alpha_)\n",
    "    model_alpha.append(model.alpha_)\n",
    "    print(model.intercept_)\n",
    "    model_intercept.append(model.intercept_)\n",
    "    \n",
    "    ypred = model.predict(xtest)\n",
    "    score = model.score(xtest, ytest)\n",
    "    ev = explained_variance_score(ytest, ypred)\n",
    "    mse = mean_squared_error(ytest, ypred)\n",
    "    print(\"R2:{0:.3f}, MSE:{1:.2f}, RMSE:{2:.2f}\"\n",
    "          .format(score, mse, np.sqrt(mse)))\n",
    "    \n",
    "    \n",
    "    ypred_cf.append(ypred)\n",
    "    score_cf.append(score)\n",
    "    ev_cf.append(ev)\n",
    "    mse_cf.append(mse)\n",
    "    coefs_cf.append(model.coef_)\n",
    "    \n",
    "combined_elastic_net = pd.DataFrame(list(zip(score_cf, ev_cf, mse_cf, model_intercept)),\n",
    "                                    columns=['R2','EV', 'MSE', 'Model_Intercept']) \n",
    "combined_elastic_net.to_csv(os.path.join(save_dir,('Depression_BDI_Elastic_Net.csv')))\n",
    "\n",
    "\n",
    "mean_coefs = np.mean(coefs_cf, axis = 0)\n",
    "ptksave(mean_coefs, os.path.join(save_dir,('Depression_BDI_mean.nii.gz')), example=ex_nii, mask=mask_nii)\n",
    "\n",
    "\n",
    "median_coefs = np.median(coefs_cf, axis = 0)\n",
    "ptksave(median_coefs, os.path.join(save_dir,('Depression_BDI_median.nii.gz')), example=ex_nii, mask=mask_nii)\n",
    "\n",
    "\n",
    "coefs_cf = np.array(coefs_cf)\n",
    "freq_coefs = (coefs_cf > 0.0001).sum(axis=0) >= 5\n",
    "ptksave(coefs_cf, os.path.join(save_dir,('Depression_BDI_coefs.nii.gz')), example=ex_nii, mask=mask_nii)\n",
    "\n",
    "binary_mask = (coefs_cf > 0.005).sum(axis=0) >= 5\n",
    "ptksave(binary_mask, os.path.join(save_dir,('Depression_BDI_gt5_mask.nii.gz')), example=ex_nii, mask=mask_nii)\n",
    "count_array = (coefs_cf > 0.0001).sum(axis=0)\n",
    "ptksave(count_array, os.path.join(save_dir,('Depression_BDI_count.nii.gz')), example=ex_nii, mask=mask_nii)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e736ff-3ed0-4fd9-9544-5d87be74290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "EPSILON = 1e-4  # This is to avoid division by zero while taking the base 10 logarithm\n",
    "fpath = Path(\"/project_cephfs/3022017.06/ENIGMA_ANX/arial.ttf\")\n",
    "\n",
    "plt.figure()\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = 5, 5\n",
    "plt.semilogx(model.alphas_ + EPSILON, model.mse_path_, ':')\n",
    "plt.plot(model.alphas_ + EPSILON, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(model.alpha_ + EPSILON, linestyle='--', color='k',\n",
    "            label=r'$\\alpha$: CV')\n",
    "plt.legend()\n",
    "plt.yticks(font = fpath)\n",
    "plt.xticks(size = 8, font = fpath)\n",
    "plt.xlabel(r'$\\alpha$', font = fpath)\n",
    "plt.ylabel('Mean square error', font = fpath)\n",
    "plt.title('Mean square error on each fold: coordinate descent ', font = fpath)\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "plt.savefig('/project_cephfs/3022017.06/ENIGMA_ANX/Z_stat/plots/Depression_BDI_ElasticNetCV5.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eead7de-f188-4dc4-b211-0d7c124b0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%PLOT ANXIETY SCORES\n",
    "# Set up the font path\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "# Assuming unique is a NumPy array or a list\n",
    "unique = np.array(unique)\n",
    "\n",
    "# Create an array of ones with the same length as unique\n",
    "y_values = np.ones(len(unique))\n",
    "\n",
    "\n",
    "fpath = Path(\"/project_cephfs/3022017.06/ENIGMA_ANX/arial.ttf\")\n",
    "\n",
    "# Set up the figure and axis\n",
    "sns.set_style(\"white\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = 5, 1\n",
    "fig, ax = plt.subplots()\n",
    "# Plot the stacked bar chart\n",
    "plt.ylim=(0.95, 1.05)\n",
    "sns.scatterplot(x = unique, y = y_values, y_jitter = 10, size = (counts/sum(counts)*100), alpha = 0.5)# palette=dict('1'=\"#B7092C\", '2'=\"#F7B192\", '3'=\"#B9D0FA\", '4'='#4052CB'), )\n",
    "\n",
    "\n",
    "#Add line for clinical threshold\n",
    "#Mapping: \n",
    "#                            {'NA' : 'NA', \n",
    "#                            'stai-trait' : 40,  20-80\n",
    "#                            'other (masq - anxious arousal)' : ,  \n",
    "#                            'stai-state' : ,\n",
    "#                            'hamilton anxiety': 17, 0-56\n",
    "#                            'other (scared total score)' : 25,  0-82\n",
    "#                            'other (dass-21_anxiety subscale)': \n",
    "#                            'other (staic)': , \n",
    "#                            'beck anxiety inventory': , \n",
    "#                            'others': }  \n",
    "\n",
    "plt.axvline(x = 17,    # Line on x = clinical cut off\n",
    "           ymin = 0, # Bottom of the plot\n",
    "           ymax = 1, \n",
    "           color = \"#4052CB\",\n",
    "           linestyle = \"--\",\n",
    "           linewidth = 0.75) # Top of the plot\n",
    "\n",
    "# Add labels and title\n",
    "plt.yticks(font = fpath)\n",
    "plt.xticks(size = 8, font = fpath)\n",
    "ax.set_xticks([0, 10, 20, 30, 40, 50])\n",
    "#ax.set_xticks([0, 10, 20, 30, 40, 50, 60, 70, 80])\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "ax.set_xlabel('Anxiety Score', font = fpath, size = 8)\n",
    "sns.despine(top=True, left=True, bottom=False, right = True)\n",
    "\n",
    "plt.legend(loc = 'best', prop=fpath, ncol=5)\n",
    "#plt.legend().set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "fig.savefig('/project_cephfs/3022017.06/ENIGMA_ANX/Z_stat/plots/Depression_BDI_withlegend.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
